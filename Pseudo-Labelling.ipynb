{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Labelling : A Semi-Supervised learning technique\n",
    "\n",
    "We will use the [Big Mart Data](https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/Sales) problem from AV data hack platform.\n",
    "\n",
    "Start by importing the basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T18:16:03.962348Z",
     "start_time": "2018-08-25T18:15:56.201741Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T18:16:04.224351Z",
     "start_time": "2018-08-25T18:16:04.040351Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/big_mart_sales/Train.csv')\n",
    "test = pd.read_csv('../data/big_mart_sales/Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take care of some preprocessing: fill NaNs, drop unique labels, categorize features, and transform the years established variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T18:16:04.777350Z",
     "start_time": "2018-08-25T18:16:04.645357Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine train and test for preprocessing steps\n",
    "combi = train.append(test, sort = False)\n",
    "\n",
    "# fill missing values with mean for item weight\n",
    "combi['Item_Weight'].fillna((combi['Item_Weight'].mean()), inplace=True)\n",
    "\n",
    "# reducing fat content to only two categories \n",
    "combi['Item_Fat_Content'] = combi['Item_Fat_Content'].replace(['low fat','LF'], ['Low Fat','Low Fat']) \n",
    "combi['Item_Fat_Content'] = combi['Item_Fat_Content'].replace(['reg'], ['Regular']) \n",
    "\n",
    "# calculating years established from establishment year\n",
    "combi['Outlet_Establishment_Year'] = 2018 - combi['Outlet_Establishment_Year'] \n",
    "\n",
    "# fill missing values for size\n",
    "combi['Outlet_Size'].fillna('Small',inplace=True)\n",
    "\n",
    "# label encoding categirical variables.\n",
    "col = ['Outlet_Size','Outlet_Location_Type','Outlet_Type','Item_Fat_Content']\n",
    "le = LabelEncoder()\n",
    "for i in col:\n",
    "    combi[i] = le.fit_transform(combi[i].astype('str'))\n",
    "    combi[i] = combi[i].astype('int')\n",
    "    \n",
    "# Split the combined array back into test and train\n",
    "train = combi[:train.shape[0]].copy()\n",
    "test = combi[train.shape[0]:].copy()\n",
    "\n",
    "## removing unique id variables from moel building arrays\n",
    "training  = train.drop(['Outlet_Identifier','Item_Type','Item_Identifier'],axis=1)\n",
    "testing  = test.drop(['Outlet_Identifier','Item_Type','Item_Identifier'],axis=1)\n",
    "\n",
    "# Setup arrays for sklearn models\n",
    "y_train = training['Item_Outlet_Sales']\n",
    "training.drop('Item_Outlet_Sales',axis=1,inplace=True)\n",
    "testing.drop('Item_Outlet_Sales',axis=1,inplace=True)\n",
    "features = training.columns\n",
    "target = 'Item_Outlet_Sales'\n",
    "\n",
    "X_train, X_test = training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model exploration\n",
    "\n",
    "We will benchmark the PL algorithm with some standard sklearn models. First we import the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T18:16:10.069865Z",
     "start_time": "2018-08-25T18:16:08.509318Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\compurob\\AppData\\Local\\conda\\conda\\envs\\ssl-env\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import BayesianRidge, Ridge, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a model factory to chug through the models with our data set using cross fold validation to score the perfomance of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T18:18:27.840432Z",
     "start_time": "2018-08-25T18:18:22.120859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor              CV-5 RMSE:  1083.15 (+/- 35120.40)\n",
      "Ridge                     CV-5 RMSE:  1206.14 (+/- 86014.33)\n",
      "ElasticNet                CV-5 RMSE:  1259.05 (+/- 115278.93)\n",
      "KNeighborsRegressor       CV-5 RMSE:  1227.93 (+/- 65213.30)\n",
      "BayesianRidge             CV-5 RMSE:  1206.26 (+/- 85051.90)\n",
      "ExtraTreesRegressor       CV-5 RMSE:  1204.44 (+/- 75170.43)\n",
      "RandomForestRegressor     CV-5 RMSE:  1187.15 (+/- 52115.88)\n",
      "GradientBoostingRegressor CV-5 RMSE:  1086.53 (+/- 35982.21)\n"
     ]
    }
   ],
   "source": [
    "model_factory = [\n",
    "    XGBRegressor(n_jobs=1),\n",
    "    Ridge(), ElasticNet(),\n",
    "    KNeighborsRegressor(),\n",
    "    BayesianRidge(),\n",
    "    ExtraTreesRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor()    \n",
    "]\n",
    "\n",
    "for model in model_factory:\n",
    "    model.random_state = 42\n",
    "    num_folds = 5\n",
    "\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')\n",
    "    score_description = \" %0.2f (+/- %0.2f)\" % (np.sqrt(scores.mean()*-1), scores.std() * 2)\n",
    "\n",
    "    print('{model:25} CV-{num_folds} RMSE: {score}'.format(\n",
    "        model=model.__class__.__name__,\n",
    "        num_folds=num_folds,\n",
    "        score=score_description\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that XGBoost performs the best and so we will use this model to test our PL algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T18:18:39.525340Z",
     "start_time": "2018-08-25T18:18:29.039648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor              CV-5 RMSE: 1083.1475 (+/- 35120.4032)\n"
     ]
    }
   ],
   "source": [
    "## normal submission using xgb\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "## saving file\n",
    "sub = pd.DataFrame(data = pred, columns=['Item_Outlet_Sales'])\n",
    "sub['Item_Identifier'] = test['Item_Identifier']\n",
    "sub['Outlet_Identifier'] = test['Outlet_Identifier']\n",
    "#sub.to_csv('bigmart-xgb.csv', index='False')\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error', n_jobs=8)\n",
    "\n",
    "# Print results\n",
    "score_description = \"RMSE: %0.4f (+/- %0.4f)\" % (np.sqrt(scores.mean()*-1), scores.std() * 2)\n",
    "\n",
    "print('{model:25} CV-{num_folds} {score_cv}'.format(\n",
    "    model=model.__class__.__name__,\n",
    "    num_folds=num_folds,\n",
    "    score_cv=score_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PseudoLabeler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T18:18:49.018311Z",
     "start_time": "2018-08-25T18:18:49.001349Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "class PseudoLabeler(BaseEstimator, RegressorMixin):\n",
    "    '''\n",
    "    Sci-kit learn wrapper for creating pseudo-lebeled estimators.\n",
    "    '''\n",
    "      \n",
    "    def __init__(self, model, unlabled_data, features, target, sample_rate=0.2, random_state=42):\n",
    "        '''\n",
    "        @model - the regressor model to build the model with\n",
    "        @unlabeld_data - X features only of unlabeled data\n",
    "        @features - list of feature names\n",
    "        @target - list of y label name\n",
    "        @sample_rate - percent of samples used as pseudo-labelled data from the unlabled dataset                      \n",
    "        '''\n",
    "        assert sample_rate <= 1.0, 'Sample_rate should be between 0.0 and 1.0.'\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.random_state = random_state\n",
    "        self.model = model\n",
    "        self.model.random_state = random_state\n",
    "        self.unlabled_data = unlabled_data\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"sample_rate\": self.sample_rate,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"model\": self.model,\n",
    "            \"unlabled_data\": self.unlabled_data,\n",
    "            \"features\": self.features,\n",
    "            \"target\": self.target\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit the data using pseudo labeling.\n",
    "        '''\n",
    "        num_of_samples = int(len(self.unlabled_data) * self.sample_rate)\n",
    "        \n",
    "        # Train the model on X and y and creat the pseudo-labels\n",
    "        self.model.fit(X, y)\n",
    "        pseudo_labels = self.model.predict(self.unlabled_data[self.features])\n",
    "        \n",
    "        # Add the pseudo-labels to the test set\n",
    "        pseudo_data = self.unlabled_data.copy(deep=True)\n",
    "        pseudo_data[self.target] = pseudo_labels\n",
    "        \n",
    "        # Take a subset of the test set with pseudo-labels and append to the training set and shuffle\n",
    "        sampled_pseudo_data = pseudo_data.sample(n=num_of_samples)\n",
    "        temp_train = pd.concat([X, y], axis=1)\n",
    "        augemented_train = shuffle(pd.concat([sampled_pseudo_data, temp_train]))\n",
    "        \n",
    "        # Fit the model again with the augemented data set\n",
    "        self.model.fit(augemented_train[self.features], augemented_train[self.target])\n",
    "        return self\n",
    "    \n",
    "           \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Returns the predicted values.\n",
    "        '''\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return self.model.__class__.__name__\n",
    "    \n",
    "    def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out PsuedoLabeler \n",
    "As of now the cross_val_score is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-25T03:39:48.216Z"
    }
   },
   "outputs": [],
   "source": [
    "model = PseudoLabeler(\n",
    "    XGBRegressor(n_jobs=1),\n",
    "    X_test,\n",
    "    features,\n",
    "    target,\n",
    "    sample_rate = 0.3\n",
    ")\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_train)\n",
    "#scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error', n_jobs=8)\n",
    "display(pred)\n",
    "\n",
    "sub = pd.DataFrame(data = pred, columns=['Item_Outlet_Sales'])\n",
    "sub['Item_Identifier'] = test['Item_Identifier']\n",
    "sub['Outlet_Identifier'] = test['Outlet_Identifier']\n",
    "#sub.to_csv('pseudo-labelling.csv', index='False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing xgboost with xgb with pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-25T03:39:48.221Z"
    }
   },
   "outputs": [],
   "source": [
    "model_factory = [\n",
    "    XGBRegressor(n_jobs=1),\n",
    "    \n",
    "    PseudoLabeler(\n",
    "        XGBRegressor(n_jobs=1),\n",
    "        X_test,\n",
    "        features,\n",
    "        target,\n",
    "        sample_rate=0.3\n",
    "    ),\n",
    "]\n",
    "\n",
    "for model in model_factory:\n",
    "    model.seed = 42\n",
    "    num_folds = 8\n",
    "    \n",
    "    scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error', n_jobs=8)\n",
    "    score_description = \"MSE: %0.4f (+/- %0.4f)\" % (np.sqrt(scores.mean()*-1), scores.std() * 2)\n",
    "\n",
    "    print('{model:25} CV-{num_folds} {score_cv}'.format(\n",
    "        model=model.__class__.__name__,\n",
    "        num_folds=num_folds,\n",
    "        score_cv=score_description\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of pseudo-labelling depedendance on sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-25T03:39:48.227Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_rates = np.linspace(0, 1, 10)\n",
    "\n",
    "def pseudo_label_wrapper(model):\n",
    "    return PseudoLabeler(model, test, features, target)\n",
    "\n",
    "# List of all models to test\n",
    "model_factory = [\n",
    "    RandomForestRegressor(n_jobs=1),\n",
    "    XGBRegressor(),\n",
    "]\n",
    "\n",
    "# Apply the PseudoLabeler class to each model\n",
    "model_factory = map(pseudo_label_wrapper, model_factory)\n",
    "\n",
    "# Train each model with different sample rates\n",
    "results = {}\n",
    "num_folds = 5\n",
    "\n",
    "for model in model_factory:\n",
    "    model_name = model.get_model_name()\n",
    "    print('%s' % model_name)\n",
    "\n",
    "    results[model_name] = list()\n",
    "    for sample_rate in sample_rates:\n",
    "        model.sample_rate = sample_rate\n",
    "        \n",
    "        # Calculate the CV-3 R2 score and store it\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error', n_jobs=8)\n",
    "        results[model_name].append(np.sqrt(scores.mean()*-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-25T03:39:48.230Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 18))\n",
    "\n",
    "i = 1\n",
    "for model_name, performance in results.items():    \n",
    "    plt.subplot(3, 3, i)\n",
    "    i += 1\n",
    "    \n",
    "    plt.plot(sample_rates, performance)\n",
    "    plt.title(model_name)\n",
    "    plt.xlabel('sample_rate')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ssl-env]",
   "language": "python",
   "name": "conda-env-ssl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "338px",
    "left": "846px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
